Module 1: Introduction to Data Engineering & Python

Overview of Data Engineering
What is Data Engineering?
•	Data Engineering is the process of designing, building, and maintaining systems that collect, store, and process data.
•	It ensures raw data is usable, reliable, and accessible for data analysts, data scientists, and business users.
•	In short, data engineers make sure data flows smoothly from source → processing → storage → consumption.
Example:
Think of an e-commerce company like Amazon:
•	Raw data: website clicks, orders, payments, shipping details.
•	A data engineer builds pipelines that collect this raw data, clean it, store it in a database or data lake, and make it available for analysis.

Role of Data Engineers in Data-Centric Organizations
•	Collecting data from multiple sources (databases, APIs, sensors, logs).
•	Building ETL/ELT pipelines – Extract, Transform, Load workflows.
•	Data modeling – defining how data should be structured in warehouses.
•	Ensuring scalability & performance – handling millions of records efficiently.
•	Collaboration – working closely with data analysts and scientists.
Real-world role:
•	If a data scientist is like a chef making dishes, a data engineer is like the farmer & supply chain worker ensuring fresh ingredients arrive on time.

Real-world Examples of Data Pipelines
1.	Batch Data Pipeline – processes data at fixed intervals (hourly/daily).
o	Example: Netflix processes daily watch history to recommend new shows.
2.	Streaming Data Pipeline – processes data in real-time.
o	Example: Uber processes GPS data from riders and drivers in real time to match them.
3.	Hybrid Pipeline – a mix of batch + streaming.
o	Example: Banking transactions: daily summaries (batch) + fraud detection (real-time).

Modern Trends in Big Data
•	Cloud-based Data Engineering – using AWS, Azure, or GCP for storage and pipelines.
•	Data Lakes & Lakehouses – storing raw + structured data together.
•	Streaming-first approach – Kafka, Spark Streaming, Flink for real-time analytics.
•	Automation & Orchestration – Airflow, Prefect, Dagster for pipeline scheduling.
•	DataOps – applying DevOps principles to manage and monitor data workflows.

Python in Data Engineering
Why Python is the Go-to Language for Data Tasks
•	Simplicity & Readability – easy to learn and maintain.
•	Rich Ecosystem – Pandas, PySpark, SQLAlchemy, NumPy, etc.
•	Integration – connects easily with databases, APIs, cloud services, and big data tools.
•	Community Support – widely adopted in data engineering, data science, and ML.
Example:
•	Pandas → small to medium datasets.
•	PySpark → huge distributed datasets.
•	SQLAlchemy → interact with databases.

Introduction to the Python Ecosystem
1.	Jupyter Notebooks
o	Interactive environment for writing and running Python code.
o	Popular for prototyping, visualizations, and tutorials.
o	Example: Data exploration, testing SQL queries.
2.	IDEs (Integrated Development Environments)
o	Tools like PyCharm, VS Code, Spyder.
o	Provide debugging, auto-completion, and project management.
o	Example: Writing production-ready ETL pipeline code.
3.	Virtual Environments
o	Isolate project dependencies to avoid version conflicts.
o	Tools: venv, virtualenv, or conda.
o	Example: Project A uses Pandas v1.5, while Project B uses Pandas v2.0 – both can co-exist without conflict.
--------------------------------------------------------------------------------------------------------------------------
Setting Up the Environment

1. Installing Python
•	Why: Python is the backbone of most data engineering work.
•	How:
o	Download from python.org or use package managers (apt on Linux, brew on macOS, choco on Windows).
o	Verify installation:
o	python --version
•	Example:
If you install Python 3.11, running python --version should return:
•	Python 3.11.6

2. Setting Up Virtual Environments
Virtual environments allow each project to have its own dependencies, avoiding conflicts.
a) venv (built-in in Python)
•	Create environment:
•	python -m venv myenv
•	Activate environment:
o	Windows: myenv\Scripts\activate
o	Linux/Mac: source myenv/bin/activate
•	Install packages locally:
•	pip install pandas
b) Conda (Anaconda/Miniconda)
•	Useful when managing data science + system-level packages.
•	Create environment:
•	conda create -n myenv python=3.10
•	Activate environment:
•	conda activate myenv
•	Install:
•	conda install pandas numpy
Example Scenario:
•	Project A → needs Pandas 1.5
•	Project B → needs Pandas 2.0
With venv or conda, both can co-exist without issues.

3. Essential Tools for Data Engineers
a) Git
•	Why: Version control to track changes in code and collaborate.
•	Commands:
•	git init                 # Initialize repo
•	git add .                # Stage files
•	git commit -m "first commit"
•	git push origin main     # Push to remote repo
•	Example: Store your ETL pipeline scripts on GitHub.

b) Jupyter Notebook
•	Why: Interactive environment for experimenting with code.
•	Install:
•	pip install notebook
•	jupyter notebook
•	Example:
Explore a CSV dataset interactively before building a pipeline.

c) VS Code
•	Why: Lightweight IDE for writing production-ready code.
•	Features: Extensions (Python, Docker, Git), debugging, integrated terminal.
•	Example:
Develop a Python ETL script in VS Code and debug it step by step.

4. Shell Scripting
•	Why: Automating repetitive tasks (file handling, running jobs).
•	Example Script: (backup.sh)
•	#!/bin/bash
•	DATE=$(date +%F)
•	tar -czf backup_$DATE.tar.gz /home/user/data
•	Run with:
•	bash backup.sh
Use Case: Automating daily log backups.

5. Cron
•	Why: Schedule scripts automatically on Linux/Unix.
•	Command to edit cron jobs:
•	crontab -e
•	Example: Run backup script daily at 2 AM:
•	0 2 * * * /home/user/backup.sh
Use Case: Schedule ETL pipeline to run every night.
________________________________________
6. Version Control Systems
a) Git (Distributed VCS)
•	Popular, open-source, decentralized.
•	Developers can work offline and sync with remote repositories.
b) Mercurial (Hg)
•	Also distributed, simpler UI, less commonly used now.
•	Example commands:
•	hg init
•	hg add file.py
•	hg commit -m "first commit"
Key Difference:
•	Git is widely adopted in industry.
•	Mercurial is used in a few projects (e.g., older Mozilla repos).
________________________________________
7. Docker and Containerization
•	Why: Package your application with dependencies so it runs anywhere.
•	Concept: Containers are lightweight, portable environments.
•	Steps:
o	Write a Dockerfile:
FROM python:3.11
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "main.py"]

Build & run container:
docker build -t myapp .
docker run myapp
Example:
Run your ETL pipeline inside Docker → avoids “works on my machine” issues.
-------------------------------------------------------------------------------------------------------
Module 2: Python Fundamentals

1. Basic Syntax & Data Types
a) Variables
•	Variables are used to store values in memory.
•	No explicit declaration required (Python is dynamically typed).
name = "Alice"
age = 25

b) Numbers
•	Integers: whole numbers → 10
•	Floats: decimal values → 10.5
•	Complex: numbers with real + imaginary part → 2 + 3j
x = 10       # int
y = 3.14     # float
z = 2 + 3j   # complex

c) Strings
•	Text enclosed in single ' or double " quotes.
greet = "Hello World"
print(greet.upper())  # HELLO WORLD

d) Booleans
•	Represent truth values: True or False.
•	Used in comparisons and conditions.
is_active = True
print(5 > 3)   # True

e) Type Conversion
•	Converting one type into another.
x = "10"
y = int(x)   # convert string to integer
print(y + 5) # 15

f) Basic Input/Output
•	Input from user:
name = input("Enter your name: ")
print("Hello", name)
•	Output using print:
print("Python is awesome!")

2. Control Flows and Structures
a) Conditional Statements
•	Used to make decisions in code.
age = 18
if age >= 18:
    print("You are an adult.")
else:
    print("You are a minor.")

b) Loops
For Loop – repeat a block for each item.
for i in range(5):
    print(i)   # prints 0 to 4
While Loop – repeat until condition is false.
count = 0
while count < 3:
    print("Hello")
    count += 1

3. Data Structures
a) Strings
•	Sequence of characters.
s = "Python"
print(s[0])     # P
print(s[::-1])  # nohtyP (reversed)

b) Lists (mutable)
•	Ordered collection, changeable.
fruits = ["apple", "banana", "cherry"]
fruits.append("mango")
print(fruits)

c) Tuples (immutable)
•	Ordered collection, cannot be modified.
colors = ("red", "blue", "green")
print(colors[1])  # blue

d) Dictionaries
•	Key-value pairs.
student = {"name": "Alice", "age": 20}
print(student["name"])  # Alice

e) Sets
•	Unordered collection, no duplicates.
nums = {1, 2, 3, 3, 4}
print(nums)  # {1, 2, 3, 4}

f) Mutability & Performance Trade-offs
•	Lists → mutable, good for dynamic changes.
•	Tuples → immutable, faster & memory-efficient.
•	Sets → good for uniqueness checks, fast lookups.
•	Dictionaries → best for key-value access.

g) Sequence Operations
•	Work on strings, lists, tuples.
nums = [1, 2, 3, 4]
print(len(nums))       # length
print(nums + [5, 6])   # concatenation
print(3 in nums)       # membership check
print(nums[1:3])       # slicing [2,3]

4. Console Interactions
a) Input and Output Modes
•	input() → read from console.
•	print() → write to console.
Example:
name = input("Enter your name: ")
print("Hello", name)

b) Console
•	Default input/output device (your terminal or shell).
•	The console (also called terminal or shell) is the default input/output device in Python.
•	By default:
o	Input is read from the keyboard.
o	Output is displayed on the screen.
Example:
name = input("Enter your name: ")   # input from console (keyboard)
print("Hello", name)                # output to console (screen)
 Here:
•	When you type your name, Python reads it from the console input.
•	print() sends the message to the console output.

c) File
•	Read/write from text files.
with open("data.txt", "w") as f:
    f.write("Hello File!")
with open("data.txt", "r") as f:
    print(f.read())

d) Streams
•	Input/output channels beyond console & file (e.g., network).
•	Example:
import sys
print("Error message", file=sys.stderr)

e) Table
•	Displaying structured data (using tabulate library).
from tabulate import tabulate
data = [["Alice", 25], ["Bob", 30]]
print(tabulate(data, headers=["Name", "Age"]))

